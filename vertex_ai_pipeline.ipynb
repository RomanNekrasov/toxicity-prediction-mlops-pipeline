{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "! pip3 install --user --no-cache-dir --upgrade \"kfp>2\" \"google-cloud-pipeline-components>2\" \\\n",
    "                                        google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing\n",
    "from typing import Dict\n",
    "from typing import NamedTuple\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component,\n",
    "                        OutputPath,\n",
    "                        InputPath)\n",
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "PROJECT_ID = \"assignment1-402316\"\n",
    "# The region that this pipeline runs in\n",
    "REGION = \"us-central1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "PIPELINE_ROOT = \"gs://temp_de2023_group1\"\n",
    "# image registry location\n",
    "IMAGE_REG = \"image-repo-group1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.container_component\n",
    "def toxic_data_ingestion(project: str, bucket: str, data_file_name: str,  features: Output[Artifact]):\n",
    "\n",
    "    return dsl.ContainerSpec(\n",
    "        image=f'{REGION}-docker.pkg.dev/{PROJECT_ID}/{IMAGE_REG}/toxic-data-ingestor:0.0.1',\n",
    "        command=[\n",
    "            'python3', '/pipelines/component/src/component.py'\n",
    "        ],\n",
    "        args=['--project_id',project,'--bucket',bucket,'--file_name',data_file_name,'--feature_path', features.path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.container_component\n",
    "def toxic_data_cleaning(features: Input[Artifact], X_dtm: Output[Artifact], y: Output[Artifact]):\n",
    "\n",
    "    return dsl.ContainerSpec(\n",
    "        image=f'{REGION}-docker.pkg.dev/{PROJECT_ID}/{IMAGE_REG}/toxic-data-cleaner:0.0.1',\n",
    "        command=[\n",
    "            'python3', '/pipelines/component/src/component.py'\n",
    "        ],\n",
    "        args=['--dataset',features.path, '--X_dtm', X_dtm.path, '--y', y.path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.container_component\n",
    "def multilabel_classifier(project: str, X_dtm: Input[Artifact], y: Input[Artifact], model_bucket: str,  metrics: OutputPath(str)):\n",
    "\n",
    "    return dsl.ContainerSpec(\n",
    "        image=f'{REGION}-docker.pkg.dev/{PROJECT_ID}/{IMAGE_REG}/toxic-multilabel-trainer:0.0.1',\n",
    "        command=[\n",
    "            'python3', '/pipelines/component/src/component.py'\n",
    "        ],\n",
    "        args=['--project_id',project,'--X_dtm', X_dtm.path, 'y', y.path,'--model_repo',model_bucket,'--metrics_path', metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"toxic-predictor-mlp\")\n",
    "def pipeline(project_id: str, data_bucket: str, trainset_filename: str):\n",
    "    \n",
    "    # The first step    \n",
    "    di_op = toxic_data_ingestion(\n",
    "        project=project_id,\n",
    "        bucket=data_bucket,\n",
    "        data_file_name=trainset_filename\n",
    "    )\n",
    "\n",
    "    # The second step \n",
    "    cleaning_op = toxic_data_cleaning(\n",
    "        features=di_op.outputs['features']\n",
    "    )\n",
    "    \n",
    "    #roman help: waarom is dit fout?\n",
    "    # The third step\n",
    "    training_op = multilabel_classifier(\n",
    "        project=project_id,      \n",
    "        X_dtm=cleaning_op.outputs['X_dtm']\n",
    "        y=cleaning_op.outputs['y']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='toxic_predictor_mlp.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "#roman help: wat is dit / hebben wij dit gedaan? (GOOGLE_APPLICATION_CREDENTIALS)\n",
    "# Before initializing, make sure to set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "# environment variable to the path of your service account.\n",
    "\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"toxic-predictor-mlp-pipeline\",\n",
    "    template_path=\"toxic_predictor_mlp.yaml\",\n",
    "    enable_caching=False,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID,\n",
    "        'data_bucket': 'data_de2023_group1', \n",
    "        'trainset_filename': 'train.csv',\n",
    "        'model_repo':'models_de2023_group1' \n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
